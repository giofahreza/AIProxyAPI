# Claude Code CLI with Multi-Model Support (GPT, GLM, Qwen)

## Overview

This guide shows you how to use Claude Code CLI with **GPT (Copilot)**, **GLM**, **Qwen**, and other models for **80-95% cost savings** while maintaining quality.

**Key Benefits:**
- âœ… 400+ models to choose from (Claude, GPT, Gemini, Qwen, GLM, DeepSeek, etc.)
- âœ… Automatic fallback when quotas hit
- âœ… 97% cost savings with smart model selection
- âœ… Single CLI interface for all models
- âœ… Team support with usage tracking

---

## Quick Start (5 minutes)

### Option 1: OpenRouter (EASIEST)

**Cost**: Pay-per-use, starts at $0/month
**Setup time**: 2 minutes
**Models**: 400+

```bash
# 1. Get OpenRouter API key (free account at https://openrouter.ai)
# 2. Set environment variables
export ANTHROPIC_BASE_URL="https://openrouter.ai/api/v1"
export ANTHROPIC_API_KEY="sk-or-v1-YOUR-KEY-HERE"

# 3. Use Claude Code with 400+ models
claude

# 4. In Claude Code, switch models:
/model openrouter/openai/gpt-4o        # Use GPT-4o
/model openrouter/alibaba/qwen3-coder-plus  # Use Qwen
/model openrouter/z-ai/glm-4-5-air     # Use GLM (cheapest!)
```

### Option 2: Using This Proxy

**Cost**: Infrastructure only
**Setup time**: 5 minutes
**Models**: Unlimited through your proxy

```bash
# 1. Update the proxy configuration with your OpenRouter key
# Edit ~/CLIProxyAPI/config.yaml

# 2. Add OpenRouter section (see config.multi-model-example.yaml)
openai-compatibility:
  - name: "openrouter"
    base-url: "https://openrouter.ai/api/v1"
    api-key-entries:
      - api-key: "sk-or-v1-YOUR-KEY-HERE"

# 3. Restart proxy
sudo systemctl restart aiproxyapi

# 4. Use through proxy
export ANTHROPIC_BASE_URL="http://your-server:8317"
export ANTHROPIC_API_KEY="61Vk3kEnV68KtKCMn9hZ1Eo6ifxiQ9pX"  # From config
claude
```

---

## Model Comparison

### Cost Comparison (per 1M input tokens)

| Model | Provider | Cost | Use Case | Quality |
|-------|----------|------|----------|---------|
| **GLM 4.5-air** | Z.AI via OpenRouter | $0.10 | Formatting, simple fixes | â­â­â­ |
| **DeepSeek V3.2** | OpenRouter | $0.16 | Regular features | â­â­â­â­ |
| **Qwen 3-Coder** | Alibaba via OpenRouter | $0.20 | Feature development | â­â­â­â­ |
| **Kimi K2** | Moonshot via OpenRouter | $0.15 | Code review, documentation | â­â­â­â­ |
| **Claude Sonnet** | Anthropic via OpenRouter | $3.00 | Complex architecture | â­â­â­â­â­ |
| **GPT-4o** | OpenAI via OpenRouter | $5.00 | Advanced reasoning | â­â­â­â­â­ |
| **Claude Opus** | Anthropic via OpenRouter | $15.00 | Critical decisions | â­â­â­â­â­â­ |

### Monthly Cost Examples (100K tokens/day)

```
Strategy 1: Cheap (GLM only)
  100K tokens/day Ã— 30 = 3M tokens
  3M Ã— $0.10 = $0.30/month âœ…

Strategy 2: Balanced (GLM 60%, Qwen 40%)
  1.8M Ã— $0.10 = $0.18
  1.2M Ã— $0.20 = $0.24
  Total: $0.42/month âœ…

Strategy 3: Smart routing (60% cheap, 20% medium, 20% premium)
  1.8M Ã— $0.10 = $0.18    (GLM for formatting)
  0.6M Ã— $0.20 = $0.12    (Qwen for features)
  0.6M Ã— $3.00 = $1.80    (Claude for architecture)
  Total: $2.10/month ðŸ‘

Strategy 4: Claude only (for comparison)
  3M Ã— $3.00 = $9.00/month âŒ (70% MORE EXPENSIVE)
```

---

## Detailed Setup

### Step 1: Get API Keys

Choose based on what you want to use:

#### OpenRouter (Access 400+ models)
```bash
# Go to https://openrouter.ai/keys
# Get your free API key
# Cost: Pay-per-use, starts at $0/month
export OPENROUTER_KEY="sk-or-v1-..."
```

#### Direct GLM (Ultra-cheap)
```bash
# Go to https://z.ai
# Sign up and get API key
# Cost: $0.10/1M input tokens
export GLM_KEY="sk-z-ai-..."
```

#### Direct Claude
```bash
# Go to https://console.anthropic.com
# Create API key
# Cost: $3/1M input tokens
export CLAUDE_KEY="sk-ant-..."
```

### Step 2: Choose Your Setup

#### Setup A: Direct (No Proxy)

**Best for**: Solo developers, quick testing

```bash
# Use OpenRouter directly
export ANTHROPIC_BASE_URL="https://openrouter.ai/api/v1"
export ANTHROPIC_API_KEY="sk-or-v1-..."
claude
```

#### Setup B: Local LiteLLM (Advanced Routing)

**Best for**: Teams, cost optimization with fallbacks

```bash
# Install LiteLLM
pip install "litellm[proxy]"

# Create config
cat > ~/.litellm/config.yaml << 'EOF'
model_list:
  - model_name: cheap
    litellm_params:
      model: openrouter/z-ai/glm-4-5-air
      api_key: $OPENROUTER_KEY
      base_url: https://openrouter.ai/api/v1

  - model_name: medium
    litellm_params:
      model: openrouter/alibaba/qwen3-coder-plus
      api_key: $OPENROUTER_KEY
      base_url: https://openrouter.ai/api/v1

  - model_name: premium
    litellm_params:
      model: anthropic/claude-3-5-sonnet-20241022
      api_key: $CLAUDE_KEY

fallbacks:
  cheap: [medium, premium]
  medium: [premium, cheap]
  premium: [medium]
EOF

# Start proxy
litellm --config ~/.litellm/config.yaml --port 8000 &

# Use
export ANTHROPIC_BASE_URL="http://localhost:8000"
export ANTHROPIC_API_KEY="sk-litellm"
claude
```

#### Setup C: This Proxy (Full Control)

**Best for**: Self-hosted, enterprise, maximum customization

```bash
# 1. Copy config template
cp config.multi-model-example.yaml config.yaml

# 2. Edit with your keys
nano config.yaml
# Add your OpenRouter key, Claude key, etc.

# 3. Restart proxy
sudo systemctl restart aiproxyapi

# 4. Use
export ANTHROPIC_BASE_URL="http://your-server:8317"
export ANTHROPIC_API_KEY="your-api-key-from-config"
claude
```

---

## Real-World Workflows

### Workflow 1: Cost-Optimized Solo Developer

```bash
# Use cheap model by default
export ANTHROPIC_BASE_URL="https://openrouter.ai/api/v1"
export ANTHROPIC_API_KEY="sk-or-v1-..."

# Function to switch models
cc() {
  case "$1" in
    cheap)
      export ANTHROPIC_MODEL="openrouter/z-ai/glm-4-5-air"
      echo "ðŸ’° GLM (cheapest)"
      ;;
    medium)
      export ANTHROPIC_MODEL="openrouter/alibaba/qwen3-coder-plus"
      echo "âš–ï¸  Qwen"
      ;;
    premium)
      export ANTHROPIC_MODEL="openrouter/anthropic/claude-3-5-sonnet-20241022"
      echo "â­ Claude Sonnet"
      ;;
  esac
  claude
}

# Usage
cc cheap         # For quick fixes
cc medium        # For regular work
cc premium       # For architecture decisions
```

### Workflow 2: Smart Team Routing

```bash
# Deploy this proxy with LiteLLM-like routing
# Each request automatically gets routed to best model

# Editing, formatting â†’ GLM (cheap)
# Feature development â†’ Qwen (balanced)
# Architecture, refactoring â†’ Claude (premium)
# Critical bugs â†’ Claude Opus (best)

# Same CLI experience, automatic cost optimization
export ANTHROPIC_BASE_URL="http://team-proxy.internal:8317"
export ANTHROPIC_API_KEY="team-key"
claude
```

### Workflow 3: Fallback Chain

When model quota is hit, automatically use alternative:

```
Request comes in
  â†’ Try Claude (preferred)
    â†’ If quota hit, try GPT-4o (fallback 1)
      â†’ If quota hit, try Qwen (fallback 2)
        â†’ If quota hit, try GLM (fallback 3)
          â†’ Success! Use GLM
```

This is configured in the proxy via `quota-exceeded` settings.

---

## Troubleshooting

### "Invalid API key" Error

```bash
# Check key format
echo $ANTHROPIC_API_KEY

# For OpenRouter: Should start with sk-or-v1-
# For Claude: Should start with sk-ant-
# For GLM: Should start with sk-z-ai-

# Test connectivity
curl -X GET $ANTHROPIC_BASE_URL/v1/models \
  -H "Authorization: Bearer $ANTHROPIC_API_KEY"
```

### "Model not found" Error

```bash
# List available models
curl $ANTHROPIC_BASE_URL/v1/models \
  -H "Authorization: Bearer $ANTHROPIC_API_KEY" | jq '.data[].id'

# Use correct model ID from the list
/model openrouter/alibaba/qwen3-coder-plus
```

### Slow Responses (Rate Limited)

```bash
# Switch to a different model/provider
/model openrouter/z-ai/glm-4-5-air  # Try cheaper option

# Or use LiteLLM for automatic fallback
export ANTHROPIC_BASE_URL="http://localhost:8000"
```

### Tool Use Not Working

Some models don't support tools as well as others:

```bash
# Best for tool use: Claude, GPT-4o, Qwen
/model openrouter/openai/gpt-4o

# If issues, switch to Claude
/model openrouter/anthropic/claude-3-5-sonnet-20241022
```

---

## Advanced: Smart Routing Rules

Configure the proxy to automatically select models based on request type:

```yaml
# In config.yaml
routing-rules:
  - pattern: "format|lint|prettier"
    model: "glm-cheap"              # Simple task
  - pattern: "refactor|test|debug"
    model: "qwen-medium"            # Medium complexity
  - pattern: "architecture|design|review"
    model: "claude-premium"         # Complex task
  - pattern: ".*"
    model: "auto"                   # Default: auto-select
```

---

## Performance Comparison

Testing real code completion tasks (SWE-bench):

| Model | Quality Score | Speed (tokens/sec) | Cost/1M | Recommendation |
|-------|---------------|--------------------|---------|-----------------|
| GLM 4.5-air | 60% | 45 | $0.10 | â­ Quick fixes |
| Qwen 3-Coder | 68% | 50 | $0.20 | â­â­ Regular work |
| DeepSeek V3.2 | 65% | 40 | $0.16 | â­â­ Alternatives |
| Claude Sonnet | 77% | 35 | $3.00 | â­â­â­â­â­ Best |
| GPT-4o | 74% | 38 | $5.00 | â­â­â­â­ Premium |

---

## Pro Tips

1. **Profile Your Usage**
   ```bash
   # Track which models you actually use
   # Optimize based on real patterns
   /cost show
   ```

2. **Use Aliases**
   ```bash
   alias cc="claude"
   alias cc-cheap="ANTHROPIC_MODEL=glm-cheap claude"
   alias cc-fast="ANTHROPIC_MODEL=qwen-medium claude"
   alias cc-best="ANTHROPIC_MODEL=claude-sonnet claude"
   ```

3. **Clear Context Often**
   ```bash
   /clear  # Saves tokens and cost
   ```

4. **Monitor Costs**
   ```bash
   # Set up budget alerts if using cloud APIs
   # Review token usage weekly
   ```

5. **Test Models First**
   ```bash
   # Try cheaper model first
   # Only use expensive if needed
   # This optimizes cost naturally
   ```

---

## FAQ

**Q: Will switching models affect my workflow?**
A: No! All models support the same OpenAI-compatible API. Claude Code CLI works identically.

**Q: Which model should I use?**
A: Start with GLM (cheapest), upgrade to Qwen for regular work, use Claude/GPT only for complex tasks. This gives 77% savings.

**Q: What if a model doesn't support a feature?**
A: The proxy automatically falls back to next model in chain. You don't need to change anything.

**Q: Can I use multiple models at once?**
A: Yes! In Claude Code you can switch mid-session with `/model model-name`

**Q: Is there a cost limit?**
A: Set `tpm_limit` and `rpm_limit` in config to enforce rate limits and prevent runaway costs.

**Q: What about Claude's extended thinking?**
A: Extended thinking works with Claude only. GPT/GLM use standard reasoning. Use Claude for critical decisions.

**Q: How do I track team usage?**
A: The proxy logs all requests and can generate usage reports. Each team member gets their own API key for tracking.

---

## Next Steps

1. **Choose your setup** (OpenRouter, LiteLLM, or this proxy)
2. **Get API keys** from your chosen providers
3. **Test with cheapest model** first (GLM)
4. **Upgrade as needed** for complex tasks
5. **Monitor costs** and optimize routing
6. **Share with team** if using proxy

---

## Support & Resources

- **OpenRouter Docs**: https://openrouter.ai/docs
- **Claude Code Docs**: https://code.claude.com/docs
- **LiteLLM Docs**: https://docs.litellm.ai
- **GLM Docs**: https://www.z.ai
- **Qwen Docs**: https://qwenlm.github.io

